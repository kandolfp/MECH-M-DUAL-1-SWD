# Pandas {#sec-sc-pandas}

`pandas` is a framework that is best suited to work with tabular or heterogeneous data, in the contrast of `numpy` that best works with homogeneous data (all having the same type).

Similar to `numpy` it is used in many other libraries to do the data wrangling in the background like `scikit-learn`. 

Before we get started a small excursion on how to structure data.

## Tidy Data

A lot of algorithms and functions require the same _tidy_ data structure as input so there should be a common definition. 

It is common to start a section about tidy data with two quotes:

> Happy families are all alike; every unhappy family is unhappy in its own way
- Leo Tolstoy

> Like families, tidy datasets are all alike but every messy dataset is messy in its own way. Tidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning).
- Hadley Wickham

::: {.callout-note}
If you follow the tidy data principle you basically have a Codd's 3 normal form database.
:::

The basic rules are:

- every variable has its own column
- every observation/stamp/... has its own row
- every single values has its own cell

| Robot_ID	| Timestamp	| Sensor_Type	| Sensor_Value	| Unit |
| --- | --- | --- | --- | --- | --- |
| 1 |	2023-09-27 08:00:00	| Temperature	| 25.3	| °C |
| 1 |	2023-09-27 08:00:15	| Temperature	| 25.5	| °C|
| 1 |	2023-09-27 08:00:30	| Humidity	    | 45.2	| %|
| 2 |	2023-09-27 08:00:00	| Temperature	| 24.8	| °C|
| 2 |	2023-09-27 08:00:15	| Temperature	| 24.9	| °C|
| 2 |	2023-09-27 08:00:30	| Humidity	    | 46.0	| %|
| 3 |	2023-09-27 08:00:00	| Temperature	| 25.0	| °C|
| 3 |	2023-09-27 08:00:15	| Temperature	| 25.1	| °C|
| 3 |	2023-09-27 08:00:30	| Humidity	    | 45.8	| %|
: Example for tidy data

As usual you can stretch these rules a bit to save storage to

| Robot_ID	| Timestamp	| Temperature_C	| Humidity_%| 
| --- | --- | --- | --- | 
| 1	| 2023-09-27 08:00:00	| 25.3	| 45.2 |
| 1	| 2023-09-27 08:00:15	| 25.5	| 45.1 |
| 2	| 2023-09-27 08:00:00	| 24.8	| 46.0 |
| 2	| 2023-09-27 08:00:15	| 24.9	| 46.2 |
| 3	| 2023-09-27 08:00:00	| 25.0	| 45.8 |
| 3	| 2023-09-27 08:00:15	| 25.1	| 45.9 |
: Example for almost tidy data

The advantage her is that you have only one line per timestamp but you loose flexibility.
Most of the time you will have multiple processes providing different streams and you have a _raw_ vault for these streams and you process them into a format you rely on to work with further.

Now that we know how we can structure data we can go on and start with `pandas`.

## Basic structure

::: {.callout-tip}
Similar to the convention to call `numpy` by the short handle `np`, `pandas` is usually referenced by `pd`

So we will use
```{python}
import numpy as np
import pandas as pd
```
:::

The two main data structures in `pandas` are 

1. `pd.Series` for one-dimensional labelled array holding dta of any type, 
1. `pd.DataFrame` for a two-dimensional data structure that holds data like a two-dimensional array of a table with rows and columns.

```{python}
#| classes: styled-output
s = pd.Series([4, 7, np.nan, -5, 3]) # <1>
print(s)                             # <2>
print(s.array)                       # <3>
print(s.index)                      # <4>
s2 = pd.Series([4, 7, np.nan, -5, 3], index=["c", "a", "b", "d", "e"]) # <5>
print(f"{s[0] == s2["c"] = }")                     # <6>
print(f"{s2[["c", "b"]] = }")                     # <6>
es = np.exp(s)                                       # <7>
print(f"{es}")
```
1. data creation is similar to `numpy`s `ndarray`
1. the output is different as it has an index and an array
1. we can also access them independently
1. the index is by default an integer starting from 0
1. the index can be specified
1. you can access via a list of indices or a single index
1. you can compute on the series

We can take this to a more extensive example and start of with the second data type the `DataFrame`.

::: {.callout-note}
`pandas` provides a nice interface that also shows data as tables and it interacts perfectly with `iPythonNotebooks` or Quarto.

So the tables you see are generated directly from Python, in order to have a better viewing experience we therefore split up the code more extensively as usual.
:::

```{python}
area_dict = {"Vienna": 415,   "Lower Austria": 19178,
             "Styria": 16401, "Upper Austria": 11982,
             "Tyrol": 12648,  "Carinthia": 9536,
             "Salzburg": 7154,"Vorarlberg": 2601,
             "Burgenland": 3965}
pop_dict = {"Vienna": 1794770, "Lower Austria": 1636287,
            "Styria": 1221014, "Upper Austria": 1436791,
            "Tyrol": 728537,   "Carinthia": 557371,
            "Salzburg": 538258,"Vorarlberg": 378490,
            "Burgenland": 288229}
area = pd.Series(area_dict)
pop = pd.Series(pop_dict)
states = pd.DataFrame({"area": area, "population": pop})
states
```
We can access transfer it back into a `Series` by accessing it accordingly.
```{python}
# | classes: styled-output
states.iloc[0:2, :]  # <2>
states.loc[["Vienna", "Lower Austria"], "population"]  # <1>
```	
1. Select by position
1. Select by label
will both return a `Series` and you can look at a detailed description of the entire `DataFrame` for each column with 
```{python}
states.describe()
```
that already provides common properties we have seen in the [Data sets](https://kandolfp.github.io/MECH-M-DUAL-1-DBM/basics/sets.html) section of the sister lecture.
Of course we can also add new columns, if they have the same index
```{python}
states["density"] = states["population"] / states["area"]
states.sort_values(by=["density"])
```

Let us use the Munich renting dataset for further investigations, as it is significantly bigger.

We can directly load it into a `DataFrame` and view its _head_ or _tail_
```{python}
df = pd.read_csv("https://data.ub.uni-muenchen.de/2/1/miete03.asc", sep="\t")  # <1>

df.head()  # no output
df.tail()
```
1. Load a csv like file.

Of course we can transfer it into a `numpy` array.
```{python}
# | classes: styled-output
df.to_numpy()
```
But this has the potential downside to cast everything to one type even as we have here clearly integers and floats mixed.
```{python}
# | classes: styled-output
df.dtypes
```
As we can see the `bj` is a `float64` while it should be a `int64` we can fix this by calling
```{python}
# | classes: styled-output
df = df.astype({"bj": "int64"})
df.dtypes
```
```{python}
# | classes: styled-output
df = df.astype({"bj": "int64"})
df.dtypes
```
We can also _transpose_ a `DataFrame` in the same style as we do in `numpy`
```{python}
df.T
```
but note that this might mess up your data types
```{python}
# | classes: styled-output
df.T.dtypes
```
We can also use boolean indexing to filter by certain conditions
```{python}
# Filter for no central heating and no warm water
filter = (df["ww0"] == 1) & (df["zh0"] == 1)
df[filter]
```

We can also use several functions to deal with missing data:

- `df.dropna(how="any")` deletes all rows that contain a `NaN` anywhere
- `df.fillna(value=1)` fills all occurring `NaN` with a value (in this case `1`) 
- `pd.isna(df)` will return a boolean mask for all `NaN` entries.

With user defined functions we can apply properties along specific axis
```{python}
df.agg(["sum", "min", "mean"], axis=0)
```
and transform, i.e. manipulate data
```{python}
# | classes: styled-output
df.groupby("bj")["bj"].transform(lambda x: x - 1900)  # <1>
```
1. `groupby` will split the data into groups based on the criteria of `Date`.

Quite often it is useful to find out how often a value appears, this can be done by
```{python}
# | classes: styled-output
df["ww0"].value_counts()
```

Of course we can combine or merge `Series` and `DataFrame` objects. 

The handling is similar as with databases where there are different ways to _ join_ data.
First we `concat` row-wise

```{python}
sections = [df[:3], df[3:7], df[1000:1001]]
pd.concat(sections)
```

To illustrate a simple `join` we define some new `DataFrames`
```{python}
left = pd.DataFrame({"key": ["foo", "foo"], "lval": [1, 2]})
right = pd.DataFrame({"key": ["foo", "foo"], "rval": [4, 5]})
pd.merge(left, right, on="key")
```
The entire list on how this can be performed in a SQL like fashion can be found [here](https://pandas.pydata.org/docs/user_guide/merging.html#merging-join).

Before we finish up our excursion into `pandas` we also want to show the plotting possibilities. 

```{python}
#| fig-cap: "Box plot of the total rent"
%config InlineBackend.figure_formats = ['svg']
df.boxplot(column=["nm"])
```

We can also compute the correlation between two series
```{python}
#| classes: styled-output
df["nm"].corr(df["wfl"])
```
or the entire `DataFrame`
```{python}
df.corr()
```


There are a lot of additional features contained in `pandas` that we will learn by using it. 
Nevertheless let us recall some further references:

- [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html)
- @McKinney2022-qz (direct link to the [Pandas section](https://wesmckinney.com/book/pandas-basics))
- [7 unique pandas functions to elevate your analysis](https://medium.com/learning-data/7-unique-pandas-functions-to-elevate-your-analysis-262fcb0db7be)